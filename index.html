<!DOCTYPE html>
<html>

<head>
    <title>UniUGP</title>
    <link rel="icon" href="website/images/logo2.png" type="image/icon type">

    <meta name="viewport" content="width=device-width, initial-scale=1">

    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels@2.0.0"></script>
    <script
        src="https://cdn.jsdelivr.net/npm/chartjs-plugin-annotation@3.0.1/dist/chartjs-plugin-annotation.min.js"></script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="website/css/bulma.min.css">
    <link rel="stylesheet" href="website/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="website/css/bulma-slider.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="./website/javascript/bulma-carousel.min.js"></script>
    <script src="./website/javascript/bulma-slider.min.js"></script>

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
        crossorigin="anonymous"></script>

    <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bootstrap4.min.css" rel="stylesheet">
    <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script>
    <!-- <script src="website/javascript/peity-vanilla.js"></script> -->

    <script src="website/javascript/benchmark_table.js" type="module"></script>
    <script src="website/javascript/success_rate_vs_k_vis.js" type="module"></script>
    <script src="website/javascript/success_rate_vs_k_vis2.js" type="module"></script>
    <script src="website/javascript/feedback_success_rate_vis.js" type="module"></script>
    <script src="website/javascript/feedback_provider_efficacy.js" type="module"></script>
    <script src="website/javascript/demos.js" type="module"></script>

    <link rel="stylesheet" href="website/css/index.css">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C7GJ4FYMY9"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'G-C7GJ4FYMY9');
    </script>

    <noscript>
        <p><img alt="Clicky" width="1" height="1" src="//in.getclicky.com/101339888ns.gif" /></p>
    </noscript>
</head>


<body>
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title publication-title">
                            <img src="website/images/logo.png" alt="logo" width="600" height="40" /><br><br>
                            <img src="website/images/logo2.png" alt="logo" width="100" height="40" /><br><br>
                            UniUGP: Unifying Understanding, Generation, and Planning For End-to-end Autonomous Driving
                        </h1>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="abstract">
        <div class="container is-max-desktop">
            <!-- Abstract -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. <br> 
                            In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations. <br>  
                        </p>
                    </div>
                </div>
            </div>
            <!-- /Abstract -->

            <!-- Method -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Method</h2>
                    <div class="content has-text-justified">
                        <p>
                            UniUGP is a unified Hybrid Expert framework for end-to-end autonomous driving, with three core experts (Understanding, Planning, Generation). It makes full use of pre-trained VLMs and generative models’ existing knowledge, uses the MoT architecture and reflected flow for advanced trajectory planning, and retains pre-trained video models’ visual causal ability when expanding generation capabilities. <br>  
                        </p>
                        <img src="website/images/pic1.png" alt="Illustration of UniUGP"
                                style="margin: 0 auto; display: block; max-width: 1000px; width: 100%; height: auto;" /><br>
                        <p>
                            Fig.1 Illustration of UniUGP, a unified model with three hybrid experts. The understanding expert performs the next-token prediction for causal reasoning. The planning expert forms a MoT architecture with the understanding expert, and performs the velocity prediction in flow matching for production future actions. The generation expert is cascaded as a world model to produce future videos. <br>  
                        </p>
                    </div>
                </div>
            </div>
            <!-- /Method -->

            <!-- Dataset -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Dataset</h2>
                    <div class="content has-text-justified">
                        <p>
                            To address the limitation of existing driving benchmarks (focused on structured scenes/simulators while ignoring long-tail events), we integrated heterogeneous datasets (Waymo-E2E, DADA2000, LaF, StHa, SOM, AADV) into a unified framework aligned with four cognitive competencies: Perception & Understanding, Causal CoT Reasoning, Planning & Decision-Making, and Instruction Following. For Perception & Understanding, three subtasks were designed: 1) Small long-tailed objects: True/False questions built from dataset segmentation labels, with diverse question templates to boost generalization; 2) Long-tailed accident prediction: True/False questions based on dataset abnormal labels and timestamps, using varied prompts; 3) Long-tailed accident relationship: Multiple-choice questions where distractors are filtered/selected (random 2/4 options), shuffled, and mapped to A/B/C/D labels, with questions asking to describe the current situation. <br>  
                        </p>
                        <p>
                            For Causal CoT Reasoning, we leveraged future image sequences and ground-truth ego trajectories to ensure reasoning aligns with physical outcomes, requiring the chain to cover scene context, key interactive agents, their potential intentions, and justifications for the final driving action. Advanced VLMs were prompted with future planning results to generate accurate CoT, which was further manually calibrated. <br>  
                        </p>
                        <img src="website/images/pic2.png" alt="Dataset Construction Pipeline"
                                style="margin: 0 auto; display: block; max-width: 1000px; width: 100%; height: auto;" /><br>
                        <p>
                            Dataset Construction Pipeline. This figure depicts the pipeline of data collection (integrating multiple challenging driving datasets) and data processing (featuring four task categories: understanding,  chain-of-thought, planning, and instruction following) to train and assess the cognitive abilities of end-to-end autonomous driving models within a unified QA framework. <br>  
                        </p>
                    </div>
                </div>
            </div>
            <!-- /Dataset -->

            <!-- Training Recipe -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Training Recipe</h2>
                    <div class="content has-text-justified">
                        <p>
                            We propose a four-stage sequential training framework: Stage 1 trains only the Understanding Expert on annotated long-tail data and ImpromptuVLA to build diverse driving scenario understanding; Stage 2 trains the Generation and Planning Experts using trajectory-equipped datasets (nuScenes, NuPlan, etc.) for visual dynamics and motion planning; Stage 3 fine-tunes the Understanding Expert with a self-annotated CoT dataset to integrate causal reasoning; Stage 4 jointly trains all three experts with mixed data from Stages 1–3. <br>  
                        </p>
                        <img src="website/images/pic3.png" alt="training framework"
                                style="margin: 0 auto; display: block; max-width: 1000px; width: 100%; height: auto;" /><br>
                    </div>
                </div>
            </div>
            <!-- /Training Recipe -->

            <!-- Result -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Result</h2>
                    <div class="content has-text-justified">
                        <p><b>Reasoning ability.</b></p><br>
                        <p>
                            The world model forces VLA to learn visual causal inference, particularly focusing on distant objects to generate better future frames. This enables the VLA model to predict potential dangers in advance, thereby ensuring driving safety. <br>  
                        </p>
                        <img src="website/images/pic4.png" alt="Result"
                                style="margin: 0 auto; display: block; max-width: 1000px; width: 100%; height: auto;" /><br>
                        <p>
                            The ablation experiment on the absence or presence of world model knowledge. The world model enables the VLA to pay more attention to future causal relationships, thereby focusing on the semantics of distant objects. <br>  
                        </p>
                        <br>

                        <p><b>Generating ability.</b></p><br>
                        <p>
                            UniUGP can generate trajectories and weather control as shown in the figure. This proves that the generation capability of our method is effective. <br>  
                        </p>
                        <!--<img src="website/images/pic5.png" alt="Result"
                                style="margin: 0 auto; display: block; max-width: 1000px; width: 100%; height: auto;" /><br>-->
                        <p>
                            Trajectory-controlled generation visualization. By modifying the trajectories input into the generation model, we can control the generation of future frames in the video. This demonstrates the controllability of our generation technology. <br>  
                        </p>
                        <!--<img src="website/images/pic6.png" alt="Result"
                                style="margin: 0 auto; display: block; max-width: 1000px; width: 100%; height: auto;" /><br>-->
                        <p>
                            Video generation visualization with controllable weather conditions. Our model can generate videos of different weather conditions, which proves the efficiency of our generation model. <br>  
                    </div>
                </div>
            </div>
            <!-- /Result -->
            
        </div>

        
        
        <div class="container is-max-desktop">
            <div class="is-centered has-text-centered">
                <div class="custom-column-large">
                    <div class="is-centered custom-column-large">
                        <h4><b>AitW General</b><br>
                            Search for some good Italian restaurants</h4>
                        <div class="text-center">
                            <div class="btn-group btn-group-toggle text-center task-selector" data-toggle="buttons">
                                <button type="button" class="btn btn-outline-secondary btn-sm inline-vis-button" disabled>Visualization:</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm active" id="all-methods">All Methods</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm" id="each-method">Each Method (pause at wish)</button>
                            </div>
                        </div>
                    </div>
                        <br>
                        <!-- All video -->
                        <div id="all-video-container">
                            <video controls autoplay muted loop>
                                <source src="website/videos/general.mp4" type="video/mp4">
                            </video>
                        </div>
                    
                        <!-- Each video -->
                        <div id="each-video-container" style="display:none;">
                            <video controls autoplay muted loop>
                                <source src="website/videos/general/slice1.mp4" type="video/mp4">
                            </video>
                            <video controls autoplay muted loop>
                                <source src="website/videos/general/slice2.mp4" type="video/mp4">
                            </video>
                            <video controls autoplay muted loop>
                                <source src="website/videos/general/slice3.mp4" type="video/mp4">
                            </video>
                            <video controls autoplay muted loop>
                                <source src="website/videos/general/slice4.mp4" type="video/mp4">
                            </video>
                    </div>
                </div>
            </div>
        </div>
        <br>
        <div class="container is-max-desktop">
            <div class="is-centered has-text-centered">
                <div class="custom-column-large">
                    <div class="custom-column-large">
                        <h4><b>AitW Web Shopping</b><br>
                            Go to newegg.com, and search for "Alienware Aurora"</h4>
                        <div class="text-center">
                            <div class="btn-group btn-group-toggle text-center task-selector" data-toggle="buttons">
                                <button type="button" class="btn btn-outline-secondary btn-sm inline-vis-button" disabled>Visualization:</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm active" id="webshop-all-methods">All Methods</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm" id="webshop-each-method">Each Method (pause at wish)</button>
                            </div>
                        </div>
                    </div>
                    <br>
                    <!-- All video -->
                    <div id="all-webshop-container">
                        <video controls autoplay muted loop>
                            <source src="website/videos/webshop.mp4" type="video/mp4">
                        </video>
                    </div>
                    
                    <!-- Each video -->
                    <div id="each-webshop-container" style="display:none;">
                        <video controls autoplay muted loop>
                            <source src="website/videos/webshop/slice1.mp4" type="video/mp4">
                        </video>
                        <video controls autoplay muted loop>
                            <source src="website/videos/webshop/slice2.mp4" type="video/mp4">
                        </video>
                        <video controls autoplay muted loop>
                            <source src="website/videos/webshop/slice3.mp4" type="video/mp4">
                        </video>
                        <video controls autoplay muted loop>
                            <source src="website/videos/webshop/slice4.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">

                <h2 class="subtitle">
                    <b>DigiRL</b> solves open-ended realistic Android tasks with an novel online reinforcement learning algorithm with autonomous VLM evaluator.
                </h2>

                <div class="tab-content" id="myTabContent">
                    <div class="tab-pane fade show active" id="benchmark-table-content" role="tabpanel"
                        aria-labelledby="benchmark-table-content">

                        <div id="benchmark-table"></div>
                    </div>
                    <div class="tab-pane fade" id="eurus-code-table-content" role="tabpanel"
                        aria-labelledby="eurus-code-table-content">

                        <p class="mt-2 px-2">
                            This code subset follows the <a href="https://arxiv.org/abs/2404.02078">Eurus
                                paper</a> and contains MBPP and HumanEval.
                        </p>
                    </div>
                    <p class="mt-2 px-2">
                        This table contains the success rate across all approaches measured in the DigiRL paper. It includes performance on
                        two subsets: AitW General and AitW Web Shopping. The codename of GPT-4V we use is <i>gpt-4-vision-preview</i>
                        and the codename of Gemini-1.5-Pro is <i>gemini-1.5-pro-latest</i>.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="interaction-framework">
        <div class="container is-max-desktop">

            <div class="columns is-full-width">

                <!-- Visual Effects. -->
                <div class="column">
                    <div class="text-justified">
                        <h2 class="title is-3">DigiRL: Autonomous RL for Building a Strong Device-Control Agent</h2>

                        <h3> Why RL over the alternatives? </h3>
                        <ul class="custom-bullets">
                            <li>LLM Agent data such as device-control actions is poorly represented in the pre-training corpus of <b>Off-the-shelf proprietary VLMs</b> such as GPT4V and Gemini-1.5-Pro.</li>
                            <li><b>Supervised Fine-Tuning</b> 1) requires a large amount of human demonstration data and 2) cannot recover from degrading model performance when real websites/applications have changed. As shown in the plot below, a frozen good policy trained with prior data experiences a gradual drop in performance as the websites change over time, while the DigiRL policy constantly updates with fresh autonomous data can maintain a stable performance.</li>
                        </ul>
                    </div>
                    <div class="text-center">
                        <!-- button for visualizing offline or not -->
                        <div class="btn-group btn-group-toggle text-center task-selector" data-toggle="buttons">
                            <button type="button" class="btn btn-outline-secondary btn-sm inline-vis-button" disabled
                                id="visualize-sr-vs-k-scale-with-model-size-llama2-base-disabled">Visualization:</button>
                            <button type="button" class="btn btn-outline-secondary btn-sm active"
                                id="exc-offline">Exclude Offline Results (click to visualize)</button>
                            <button type="button" class="btn btn-outline-secondary btn-sm"
                                id="inc-offline">Include Offline Results</button>
                        </div>
                    </div>
                    <div class="text-justified">
                        <div class="chart-container" id="chart-k2" style="display:block;margin:0 auto;">
                            <canvas id="chart-sr-vs-k2"></canvas>
                        </div>
                        <h3> What are we using RL for?</h3>
                        DigiRL consists of two steps:
                        <ul class="custom-bullets">
                            <li>First, we use <b>Offline RL</b> to make the most out of a potentially sub-optimal existing offline dataset.</li>
                            <li>Then, we use <b>Offline-to-Online RL</b> to encourage the agent to learn from its own trials and errors.</li>
                        </ul>
                        DigiRL identifies the most simple yet effective RL design choices for device-control agent problems. Our RL algorithmic framework automatically achieves the following advantages compared to state-of-the-art alternatives such as rejection sampling (or Filtered Behavior Cloning):
                        <ul class="custom-bullets">
                            <li>We makes use of an <b>instruction-level value function</b> to implicitly construct an automatic curriculum that prioritizes on the tasks most informative to the agent.</li>
                            <li>We makes use of a <b>step-level value function</b> to pick out the advantageous actions (actions that mark progress towards the goal) in a trajectory while leaving the noisy actions (actions that do not contribute to the goal).</li>
                        </ul>
                        Please check out our paper for more details of our algorithm!

                        <div style="text-align:center;">
                            <img src="website/images/algo.png" alt="illustrative-example"
                                style="margin: 0 auto; display: block; max-width: 1000px; width: 100%; height: auto;" />
                            <br>
                        </div>
                        <h3>Learning Curves</h3>
                        <div class="text-justified" id="tool-augmented">
                            In addition to the convergence performance reported in the paper, we also present the sample complexity comparison of DigiRL against the state-of-the-art alternative Filtered Behavior Cloning (or rejection sampling). We find that DigiRL not only converges to a superior performance, but also learns more efficiently.
                        </div>
                        <div class="text-center">
                            <br>
                            <div class="btn-group btn-group-toggle text-center task-selector" data-toggle="buttons">
                                <button type="button" class="btn btn-outline-secondary btn-sm inline-vis-button" disabled
                                    id="visualize-sr-vs-k-scale-with-model-size-llama2-base-disabled">Subset:</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm active"
                                    id="visualize-sr-vs-k-scale-with-model-size-llama2-base">AitW General (click to visualize)</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm"
                                    id="visualize-sr-vs-k-scale-with-model-size-llama2-rlhf">AitW Web Shopping</button>
                            </div>
                        </div>
                        <div class="chart-container" id="chart-k" style="display:block;margin:0 auto;">
                            <canvas id="chart-sr-vs-k"></canvas>
                        </div>

                    </div>
                </div>
                <!--/ Visual Effects. -->

            </div>
    </section>

    <section class="section" id="evaluation">
        <div class="container is-max-desktop">

            <div class="columns is-full-width">

                <!-- Visual Effects. -->
                <div class="column">
                    <div class="content">
                        <h3 class="title is-3">Autonomous Evaluation</h3>
                        <p>
                            Our main results are autonomously evaluated with Gemini-1.5-Pro. We also manually evaluate on some subsets and finds that the autonomous evaluation results highly align with manual evaluations with an average difference less than 3%:
                        </p>

                        <div class="text-center">
                            <div class="btn-group btn-group-toggle text-center feedback-provider-sort-by-selector"
                                data-toggle="buttons">
                                <button type="button" class="btn btn-outline-secondary btn-sm" disabled>Subset:</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm active"
                                    id="sort-by-feedback-gain">
                                    AitW General
                                </button>

                                <button type="button" class="btn btn-outline-secondary btn-sm"
                                    id="sort-by-feedback-provider-perf">
                                    AitW Web Shopping
                                </button>
                            </div>
                        </div>

                        <div class="chart-container" id="chart-feedback-p" style="display:block;margin:0 auto;">
                            <canvas id="chart-feedback-provider"></canvas>
                        </div>

                        <h3>Failure Mode Analysis</h3>

                        <p>
                            While all the types of failure modes benefit from offline and offline-to-online RL training, the most consistent and significant reduction is for the failure mode of failing to recover from mistakes. By training on autonomously-collected rollouts, our agent DigiRL is able to learn from its own mistakes and reduces failures to recover over training.
                        </p>

                        <div class="text-center">
                            <div class="btn-group btn-group-toggle text-center task-selector" data-toggle="buttons">
                                <button type="button" class="btn btn-outline-secondary btn-sm" disabled>Subset:</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm active"
                                    id="avg_micro">AitW General</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm"
                                    id="reasoning">AitW Web Shopping</button>
                            </div>


                            <!-- <div class="btn-group btn-group-toggle text-center sort-by-selector" data-toggle="buttons">
                                <button type="button" class="btn btn-outline-secondary btn-sm" disabled>Failure:</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm active"
                                    id="sort-by-feedbacksr">Fail to recover from mistakes</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm"
                                    id="sort-by-nofeedbacksr">Get stuck midway</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm"
                                    id="sort-by-feedbackdelta">Arrive at wrong goal</button>
                            </div> -->
                        </div>

                        <div class="chart-container" id="chart-feedback" style="position:relative;margin:0 auto;">
                            <canvas id="chart-sr-w-feedback" style="max-height: 100%;"></canvas>
                        </div>

                    </div>
                </div>
            </div>
    </section>

    <section class="section" id="evaluation">
        <div class="container is-max-desktop">
            <div class="columns is-full-width">
                <div class="column">
                    <div class="content">
                        <h3>Misc</h3>
                        <h4>Re the Charts &#128200 </h4>
                        Try clicking on the legend of the charts!
                    </div>
                    <div class="content">
                        <h4>Re the Icon <img src="website/images/icon/icon.png" alt="logo" width="30" height="30" /></h4>
                        <p>
                            <b>Infinity:</b> Our environment is open-ended, which can be easily generalized to infinite open-ended tasks sets with our open-ended evaluator.
                            <br>
                            <b>Loop:</b> We use online reinforcement learning, which is closed-loop: the agent interacts with the environment and learns from its own trials and errors.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@article{bai2024digirl,
title={DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning},
author={Bai, Hao and Zhou, Yifei and Cemri, Mert and Pan, Jiayi and Suhr, Alane and Levine, Sergey and Kumar, Aviral},
journal={arXiv preprint arXiv:2406.11896},
year={2024}
}</code></pre>
        </div>
    </section>

    <footer class="footer">
        <div align="center" class="container">
            <div class="columns is-centered">
                <div class="content is-small">
                    This website templated is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">here</a> and <a href="https://xwang.dev/mint-bench/">here</a>.
                </div>
            </div>
        </div>
    </footer>

</body>


</html>
